

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Klasyfikacja w Python &#8212; Silky Coders Data Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '12. Klasyfikacja/Klasyfikacja';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Liniowe modele regresji" href="../11.%20Regresja/Regresja_lin.html" />
    <link rel="prev" title="Klasteryzacja w Python" href="../7.%20Klasteryzacja/7_Klasteryzacja.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/silky-logo.png" class="logo__image only-light" alt="Silky Coders Data Science - Home"/>
    <img src="../_static/silky-logo.png" class="logo__image only-dark pst-js-only" alt="Silky Coders Data Science - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Laboratorium specjalistyczne: Data Science w branży modowej, 1 semestr studiów magisterskich Matematyki na Wydziale Fizyki Technicznej i Matematyki Stosowanej na Politechnice Gdańskiej
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Fundamenty Pythona</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../0.%20Wst%C4%99p/AnacondaJupyter.html">JupyterLab &amp; Anaconda</a></li>

<li class="toctree-l1"><a class="reference internal" href="../0.%20Wst%C4%99p/TutorialBasicsPython.html">Podstawy Python</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wprowadzenie do pakietów numpy i pandas</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1.%20Tutorial%20pandas/1_Tutorial%20pandas.html">Pakiet pandas - tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2.%20Numpy_tutorial/2_Tutorial%20numpy.html">Pakiet NumPy - tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wizualizacja danych</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../3.%20Wizualizacja_danych/3_Wizualizacja%20danych.html">Wizualizacja danych w Python</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Analiza statystyczna</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4.%20Analiza_statystyczna/4_Analiza%20statystyczna.html">Analiza statystyczna - tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wykrywanie anomalii</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5.%20Wykrywanie_anomalii/Wykrywanie_anomalii_teoria.html">Wykrywanie anomalii - definicje</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5.%20Wykrywanie_anomalii/5_Wykrywanie%20anomalii%20tutorial.html">Wykrywanie anomalii - tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Inżynieria cech</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../6.%20Inzynieria_cech/6_Inzynieria%20cech.html">Inżynieria cech</a></li>











</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Czyszczenie danych</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../10.%20Czyszczenie_danych/Czyszczenie%20danych.html">Czyszczenie danych</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Klasteryzacja</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../7.%20Klasteryzacja/7_Klasteryzacja.html">Klasteryzacja w Python</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Klasyfikacja</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Klasyfikacja w Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Regresja</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../11.%20Regresja/Regresja_lin.html">Liniowe modele regresji</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11.%20Regresja/Regresja_trees.html">Regresja oparta o modele drzewiaste</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wyjaśnialność modeli</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../8.%20Wyjasnialnosc/8_Tutorial%20metryki%20statystyczne.html">Wyjaśnialność modeli - metryki statystyczne - tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../9.%20XAI/Wyjasnialnosc_modeli_definicje.html">Wyjaśnialność modeli - definicje</a></li>
<li class="toctree-l1"><a class="reference internal" href="../9.%20XAI/9_Wyjasnialnosc%20modeli.html">Wyjaśnialność modeli - XAI - tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Przetwarzanie języka naturalnego</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../13.%20NLP/TutorialNLP.html">NLP</a></li>





</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/kikonPL/studia_PG/blob/main/12. Klasyfikacja/Klasyfikacja.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/kikonPL/studia_PG" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/kikonPL/studia_PG/issues/new?title=Issue%20on%20page%20%2F12. Klasyfikacja/Klasyfikacja.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/12. Klasyfikacja/Klasyfikacja.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Klasyfikacja w Python</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wstep">Wstęp</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#macierz-pomylek">Macierz pomyłek</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#biblioteki">Biblioteki</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#przygotowanie-danych">Przygotowanie danych</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#losowe-dane">Losowe dane</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia">Bibliografia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modele-klasyfikacyjne">Modele klasyfikacyjne</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors">K-Nearest Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#przyklad">Przykład</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-k-nearest-neighbors">Bibliografia K-Nearest Neighbors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-logistic-regression">Bibliografia Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-trees">Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-decision-trees">Bibliografia Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forests">Random Forests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-random-forest">Bibliografia Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machines">Support Vector Machines</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#przyklad-z-jadrem-liniowym">Przykład z jądrem liniowym.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#przyklad-z-jadrem-radialnym">Przykład z jądrem radialnym.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-support-vector-machines">Bibliografia Support Vector Machines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dodatkowe-informacje">Dodatkowe informacje</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="klasyfikacja-w-python">
<h1>Klasyfikacja w Python<a class="headerlink" href="#klasyfikacja-w-python" title="Permalink to this heading">#</a></h1>
<hr class="docutils" />
<section id="wstep">
<h2>Wstęp<a class="headerlink" href="#wstep" title="Permalink to this heading">#</a></h2>
<p>Algorytmy klasyfikacyjne, w przeciwieństwie do klasteryzacji, należą do modeli uczenia maszynowego z nadzorem (ang. <em>supervised learning</em>), to znaczy, że w zbiorze uczącym mamy określoną zmienną celu (target) oraz zdefiniowane zmienne objaśniające. Przykładem klasyfikacji może być wykrywanie spamu w poczcie, czy też rozpoznawanie obiektów na obrazie. Algorytmów klasyfikacyjnych możemy też użyć do wykrywania anomalii np. przy pomocy algorytmu KNN.</p>
<p>Rozdzielamy klasyfikację binarną oraz wielowymiarowa, gdzie w przypadku klasyfikacji binarnej mamy zmienną objaśnianą z dwiema możliwymi wartościami (np. pacjent zdrowy lub chory). Dla wariantu klasyfikacji wielomianowej zmienna objaśniana może mieć wiele różnych wartości (np. klasyfikując owoce rozróżniamy jabłka, gruszki i pomarańcze). Klasyfikacja jest procesem przypisywania wcześniej zdefiniowanych klas, opierając się na ich atrybutach.</p>
<p>Do wyjaśnialności modeli klasyfikujących używamy głównie macierzy pomyłek (ang. <em>confusion matrix</em>), na której podstawie możemy policzyć takie metryki jak <em>accuracy</em>, <em>precision</em> czy też <em>sensitivity</em>.
Poniżej przedstawiono pięć algorytmów klasyfikacyjnych, ich działanie oraz w jakich przypadkach są one użyteczne.</p>
</section>
<section id="macierz-pomylek">
<h2>Macierz pomyłek<a class="headerlink" href="#macierz-pomylek" title="Permalink to this heading">#</a></h2>
<p>Pojęcie macierzy pomyłek, wraz z metrykami, które się na niej opierają, zostało przedstawione w rozdziale wyjaśnialności modeli (<a class="reference external" href="https://kikonpl.github.io/studia_PG/8.%20Wyjasnialnosc/8_Tutorial%20metryki%20statystyczne.html#podejscie-klasyfikacyjne">Macierz pomyłek</a>). Z tego względu w tym rozdziale ograniczono się wyłącznie do jej stosowania, bez dokładnego wytłumaczenia tego pojęcia.</p>
</section>
<hr class="docutils" />
<section id="biblioteki">
<h2>Biblioteki<a class="headerlink" href="#biblioteki" title="Permalink to this heading">#</a></h2>
<p>Wczytujemy potrzebne biblioteki.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Manipulacja danych i operacje statystyczne</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Przykladowe ramki danych</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="c1"># Wizualizacja danych</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Podział na zbiór uczący i testowy</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># liczenie metryk oceniających model</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="c1"># Klasyfikacja algorytmem k najbliższych sąsiadów</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="c1"># Klasyfikacja regresją logistyczną</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Klasyfikacja drzewami decyzyjnymi</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="c1"># Klasyfikacja lasami losowymi</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Klasyfikacja przy pomocy algorytmu support vector machines</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># Metryki oceniające model</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="przygotowanie-danych">
<h2>Przygotowanie danych<a class="headerlink" href="#przygotowanie-danych" title="Permalink to this heading">#</a></h2>
<p>W celu zaprezentowania działania algorytmów klasyfikujących, wygenerujemy przykładowy zbiór danych, zawierający 2 zmienne liczbowe oraz 1 zmienną binarną, która w tym przypadku będzie oznaczała daną klasę.</p>
<section id="losowe-dane">
<h3>Losowe dane<a class="headerlink" href="#losowe-dane" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stworzenie losowego obiektu np.array</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>         <span class="c1"># ilość danych</span>
                           <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>           <span class="c1"># liczba zmiennych objaśniających</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>        <span class="c1"># liczba &#39;użytecznych&#39; zmiennych</span>
                           <span class="n">n_redundant</span> <span class="o">=</span><span class="mi">0</span><span class="p">,</span>         <span class="c1"># liczba zmiennych &#39;nieużytecznych&#39;</span>
                           <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>            <span class="c1"># liczba klas</span>
                           <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>   <span class="c1"># zbalansowanie danych</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">]</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>

<span class="c1"># Sprawdzenie czy dane na pewno są zbalansowane</span>
<span class="n">display</span><span class="p">(</span><span class="s2">&quot;Ilość poszczególnych wartości zmiennej y&quot;</span><span class="p">,(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">30</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Typ obiektu: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Typ danych: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Wymiar obiektu array: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">30</span><span class="p">)</span>

<span class="c1"># Wizualizacja powyższego zbioru danych z wyróżnieniem zmiennej y</span>
<span class="n">sns</span><span class="o">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Wykres punktowy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Ilość poszczególnych wartości zmiennej y&#39;
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y
0    1397
1     603
Name: count, dtype: int64
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>------------------------------
Typ obiektu: &lt;class &#39;numpy.ndarray&#39;&gt;
Typ danych: float64
Wymiar obiektu array: (2000, 2)
------------------------------
</pre></div>
</div>
<img alt="../_images/174e285b325561a129668b8add47e69e40a1ca1080ee0a2aec2675415c21b07f.png" src="../_images/174e285b325561a129668b8add47e69e40a1ca1080ee0a2aec2675415c21b07f.png" />
</div>
</div>
<p>Za pomocą powyższego kodu otrzymaliśmy niezbalansowany zbiór danych, posiadający 2 klasy. 1397 punktów należy do klasy 0, a 603 do klasy 1.
W celu zaprezentowania metod działania algorytmów klasyfikacyjnych, zbiór ten zostanie podzielony na zbiór uczący i testowy w proporcjach odpowiednio 8:2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dzielimy nasze dane na zbiory uczący i testowy</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="bibliografia">
<h2>Bibliografia<a class="headerlink" href="#bibliografia" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html</a></p>
<p><a class="reference external" href="https://proclusacademy.com/blog/sklearn_make_classification/">https://proclusacademy.com/blog/sklearn_make_classification/</a></p>
</section>
<hr class="docutils" />
<section id="modele-klasyfikacyjne">
<h2>Modele klasyfikacyjne<a class="headerlink" href="#modele-klasyfikacyjne" title="Permalink to this heading">#</a></h2>
<p>W poniższym notatniku, wykorzystanych zostanie 5 algorytmów klasyfikujących, a będą to:</p>
<ul class="simple">
<li><p>K-Nearest Neighbors</p></li>
<li><p>Logistic Regression</p></li>
<li><p>Decision Trees</p></li>
<li><p>Random Forests</p></li>
<li><p>Support Vector Machines</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="k-nearest-neighbors">
<h2>K-Nearest Neighbors<a class="headerlink" href="#k-nearest-neighbors" title="Permalink to this heading">#</a></h2>
<p>Algorytm K-najbliższych sąsiadów opiera się na sprawdzaniu odległości pomiędzy pewnymi testowymi przykładami, a wartościami ze zbioru uczącego. Wybór liczby k, definiuje ilość najbliższych sąsiadów ze zbioru uczącego. Uśredniając wartości zmiennej objaśnianej dla wybranych obserwacji, otrzymujemy prognozę. Określenie “najbliższy” nie zawsze musi być jednoznaczne, w algorytmie istnieje możliwość stosowania różnych metryk liczących odległości (np. metryka euklidesowa, czy też taksówkowa).
Algorytm ten najlepiej stosować, gdy zmienne objaśniające i objaśniane nie posiadają prostych zależności między sobą (np. zależność ta nie jest liniowa).</p>
<p>Na poniżczym przykładzie, punkt testowy, oznaczony kolorem zielonym, zostanie zaklasyfikowany jako czerwony, gdy za k przyjmiemy 3 oraz jako niebieski, gdy za k weźmiemy 5.</p>
<p><img alt="Przykład" src="../_images/KNN.png" /></p>
<p><a href="https://pl.wikipedia.org/wiki/K_najbli%C5%BCszych_s%C4%85siad%C3%B3w#/media/Plik:KnnClassification.svg" target="_blank">źródło</a></p>
<p><strong>Podstawowe metryki</strong></p>
<p><strong>Metryka euklidesowa</strong></p>
<p>Metrykę euklidesową w przestrzeni <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> definiuje się wzorem
$<span class="math notranslate nohighlight">\(d_e(x,y)=\sqrt{(y_1-x_1)^2+...+(y_n-x_n)^2}\)</span>$
<strong>Metryka taksówkowa (manhattan)</strong></p>
<p>Metrykę taksówkową w przestrzeni <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> definiujemy za pomocą poniższego wzoru
$<span class="math notranslate nohighlight">\(d_m(x,y)=\sum\limits^{n}_{k=1}|x_k-y_k|\)</span>$</p>
<p><strong>Kroki działania algorytmu</strong></p>
<p>Zakładamy, że mamy podział na zbiór uczący i testowy, wtedy algorytm prezentuję się następująco</p>
<ol class="arabic simple">
<li><p>Wybór wartośći <em>k</em></p></li>
<li><p>Kalkulacja macierzy odległości pomiędzy obserwacjami</p></li>
<li><p>Określenie predykcji <em>y</em> na podstawie k-najbliższych sąsiadów</p></li>
<li><p>Kalkulacja skuteczności</p></li>
</ol>
<p>Weźmy pod uwagę przykładowe zbiory <span class="math notranslate nohighlight">\(X_{\textrm{ucz}} = [x_1=(1,3,4),\;x_2=(2,3,1),\;x_3=(3,2,3),\;x_4=(5,1,3)]\)</span>, <span class="math notranslate nohighlight">\(Y_{\textrm{ucz}}=[y_1=1,y_2=0,y_3=0,y_4=1]\)</span> oraz za wartość k przyjmijmy k=3.
Za metrykę określającą odległości pomiędzy zmiennymi wybierzemy metrykę euklidesową.
Naszą nową wartością do zaklasyfikowania będzie <span class="math notranslate nohighlight">\(x_5=(2,2,2)\)</span>
wtedy macierz odległości będzie wyglądała następująco.</p>
<p><span class="math notranslate nohighlight">\(d_e(x_1,x_5)=\sqrt{(1-2)^2+(3-2)^2+(4-2)^2}=\sqrt{6}\)</span></p>
<p><span class="math notranslate nohighlight">\(d_e(x_2,x_5)=\sqrt{(2-2)^2+(3-2)^2+(1-2)^2}=\sqrt{2}\)</span></p>
<p><span class="math notranslate nohighlight">\(d_e(x_3,x_5)=\sqrt{(3-2)^2+(2-2)^2+(3-2)^2}=\sqrt{2}\)</span></p>
<p><span class="math notranslate nohighlight">\(d_e(x_4,x_5)=\sqrt{(5-2)^2+(1-2)^2+(3-2)^2}=\sqrt{11}\)</span></p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky">d</th>
    <th class="tg-0pky">x5</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">x1</td>
    <td class="tg-0pky">&#8730 6</td>
  </tr>
  <tr>
    <td class="tg-0pky">x2</td>
    <td class="tg-0pky">&#8730 2</td>
  </tr>
  <tr>
    <td class="tg-0pky">x3</td>
    <td class="tg-0pky">&#8730 2</td>
  </tr>
  <tr>
    <td class="tg-0pky">x4</td>
    <td class="tg-0pky">&#8730 11</td>
  </tr>
</tbody>
</table><p>Ponieważ k=3, to sprawdzamy do jakich klas należą trzej najbliżsi sąsiędzi, są to <span class="math notranslate nohighlight">\(x_1,\;x_2,\;x_3\)</span> o klasach odpowiednio <span class="math notranslate nohighlight">\(1,\;0,\;0\)</span>, zatem obserwacji <span class="math notranslate nohighlight">\(x_5\)</span> zostanie przypisana klasa 0.</p>
<section id="przyklad">
<h3>Przykład<a class="headerlink" href="#przyklad" title="Permalink to this heading">#</a></h3>
<p>W celu lepszego zrozumienia algorytmu posłużymy się poniższym przykładem. Użyjemy danych wygenerowanych wcześniej do zbioru X i y, które następne zostały podzielone na zbiory treningowy i testowy. Przedstawiono 3 warianty algorytmu, dla różnej liczby sąsiadów. Patrząc na wykresy widać, że w tym przypadku większa liczba sąsiadów daje lepsze rezultaty. Jednak w celu potwierdzenia tej hipotezy najlepiej sprawdzić metryki określające wiarygodność tego modelu i wtedy wyznaczyć najoptymalniejszą liczbę sąsiadów.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">21</span><span class="p">]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">K</span><span class="p">:</span>
    <span class="c1"># Definiujemy model klasyfikujący, bazujący na algorytmie k najbliższych sąsiadów.  </span>
    <span class="n">KNN_clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>  <span class="c1"># liczba sąsiadów</span>

    <span class="c1"># Uczymy powyższy model na naszych danych</span>
    <span class="n">KNN_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Stosujemy powyższy model na danych testowych</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">KNN_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
      
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;k=&quot;</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
    <span class="c1"># Wizualizacja działania algorytmu</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">]</span>
    <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y_pred&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Wykres punktowy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="c1"># Tworzymy macierz pomyłek dla ostatnio stworzonego modelu</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision: </span><span class="si">{</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recall: </span><span class="si">{</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F1: </span><span class="si">{</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="n">KNN_clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
    <span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">KNN_clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>  
    <span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>k= 3
</pre></div>
</div>
<img alt="../_images/d778206e6b2502d8fa3b6a68e6f8e25f4d04a9e1c85869437d99d0c826c1c806.png" src="../_images/d778206e6b2502d8fa3b6a68e6f8e25f4d04a9e1c85869437d99d0c826c1c806.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.955
Precision: 0.913
Recall: 0.943
F1: 0.927
------------------------------------------------------------
k= 5
</pre></div>
</div>
<img alt="../_images/39a6be5d5af151c159a72e903220da7b09d988d8302e1d1653871020ce561308.png" src="../_images/39a6be5d5af151c159a72e903220da7b09d988d8302e1d1653871020ce561308.png" />
<img alt="../_images/276de7c7d00c4416f5754be800c97c3d8c337b821b013630a74921e8829069f7.png" src="../_images/276de7c7d00c4416f5754be800c97c3d8c337b821b013630a74921e8829069f7.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.948
Precision: 0.891
Recall: 0.943
F1: 0.916
------------------------------------------------------------
k= 21
</pre></div>
</div>
<img alt="../_images/8dcd2715e0e4c9469840a1b4c444a39203f801cde962ac09435b8dcc08e213e3.png" src="../_images/8dcd2715e0e4c9469840a1b4c444a39203f801cde962ac09435b8dcc08e213e3.png" />
<img alt="../_images/cff22cbb1da574cb4749d200abdb7f3620b0db267db9df29692521b142603995.png" src="../_images/cff22cbb1da574cb4749d200abdb7f3620b0db267db9df29692521b142603995.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.950
Precision: 0.905
Recall: 0.934
F1: 0.919
------------------------------------------------------------
</pre></div>
</div>
<img alt="../_images/0a09b28af93c4aeba4194b8341d051830d55eee3a3dfda93fc86a415068bade2.png" src="../_images/0a09b28af93c4aeba4194b8341d051830d55eee3a3dfda93fc86a415068bade2.png" />
</div>
</div>
<p>W celu wyznaczenia najoptymalniejszej wartośći k możemy sprawdzić jak zmienia się wartość metryki F1 w zależności od tej liczby. Jest to metryka, która jest dokładniejszą metryką, niż Accuracy pozwalająca zachować odpowiednia równowagę pomiędzy Recall, a Precision.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">error_rate</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">60</span><span class="p">):</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred_i</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">error_rate</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">pred_i</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">60</span><span class="p">),</span><span class="n">error_rate</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>
 <span class="n">markerfacecolor</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;F1-score vs. K Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;K&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;F1-score&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1223cb75518919ced3ec7486e9b866426bb8a2945f1e4a4fbb1faeadf48b99b1.png" src="../_images/1223cb75518919ced3ec7486e9b866426bb8a2945f1e4a4fbb1faeadf48b99b1.png" />
</div>
</div>
<p>Z powyższego wykresu widzimy, że najstabilniejszy wzrost F1-score zachodzi od k=20 do k=32, gdzie dla k=32 osiągamy maksimum, dlatego tę wartość możemy uznać za najlepszą.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">KNN_clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">KNN_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">KNN_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Tworzymy macierz pomyłek dla ostatnio stworzonego modelu</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision: </span><span class="si">{</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recall: </span><span class="si">{</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F1: </span><span class="si">{</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="n">KNN_clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">KNN_clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>  
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.960
Precision: 0.927
Recall: 0.943
F1: 0.935
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x20bf4d5bcb0&gt;
</pre></div>
</div>
<img alt="../_images/0a09b28af93c4aeba4194b8341d051830d55eee3a3dfda93fc86a415068bade2.png" src="../_images/0a09b28af93c4aeba4194b8341d051830d55eee3a3dfda93fc86a415068bade2.png" />
</div>
</div>
</section>
</section>
<section id="bibliografia-k-nearest-neighbors">
<h2>Bibliografia K-Nearest Neighbors<a class="headerlink" href="#bibliografia-k-nearest-neighbors" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</a></p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/neighbors.html#classification">https://scikit-learn.org/stable/modules/neighbors.html#classification</a></p>
<p><a class="reference external" href="https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/">https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/</a></p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a></p>
</section>
<hr class="docutils" />
<section id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this heading">#</a></h2>
<p>Jest to jeden z najważniejszych modeli z rodziny uogólnionych modeli liniowych (GLM). Regresji logistycznej używamy, gdy chcemy przewidzieć zmienną binarną. Niezależne zmienne podlegają analizie, na podstawie której określany jest rezultat działania algorytmu. Jest to algorytm liniowy, który na podstawie prawdopodobieństwa przypisuje predykcji jedną z dwóch wartości zmiennej objaśnianej. Stanowi ona dobry pierwszy wybór przy klasyfikacji binarnej. Warto wiedzieć, że regresja logistyczna jest dobrze skalibrowana, tzn. odtwarza prawdopodobieństwa marginalne danych. Nie nadaje się ona w przypadku, gdy mamy dużo zmiennych lub zmienne kategoryczne mają bardzo dużą liczbę poziomów.</p>
<p>Modele liniowe możemy opisać za pomocą wzoru <span class="math notranslate nohighlight">\(\hat{y}(w,x)=w_0+w_1x_1+...+w_px_p\)</span>, gdzie wektor <span class="math notranslate nohighlight">\(w=(w_1,...,w_p)\)</span> oznacza wektor współczynników, <span class="math notranslate nohighlight">\(w_0\)</span> to wyraz wolny (ang. <em>intercept</em>), a <span class="math notranslate nohighlight">\(\hat{y}\)</span> oznacza predykcje modelu.</p>
<p>Regresja logistyczna opiera się na pojęciu szansy (ang. <em>odds</em>). Wyraża się ją za pomocą wzoru $<span class="math notranslate nohighlight">\(Odds = \frac{p}{1-p}\)</span>$</p>
<p>Szansa, w porównanu do prawdopodobieństwa, przyjmuje dla <span class="math notranslate nohighlight">\(0&lt;p&lt;1\)</span> wartości z przedziału <span class="math notranslate nohighlight">\((0,+\infty)\)</span>, a jej logarytm wartości z zakresu <span class="math notranslate nohighlight">\((-\infty,+\infty)\)</span>. Funkcja przekształcająca prawdopodobieństwo na logarytm szansy zwana jest <em>logitem</em> i przyjmuje postać:
$<span class="math notranslate nohighlight">\(\textrm{logit}(p)=\textrm{ln}\frac{p}{1-p}\)</span>$</p>
<p>Wtedy logit nieznanego prawdopodobieństwa sukcesu <span class="math notranslate nohighlight">\(p_{i}\)</span> jest modelowany jako liniowa funkcja <span class="math notranslate nohighlight">\(x_{i}\)</span>:
$<span class="math notranslate nohighlight">\(\textrm{ln}\frac{p}{1-p}=w_0+w_1x_1+...+w_px_p\)</span>$</p>
<p><img alt="Logit" src="https://upload.wikimedia.org/wikipedia/commons/c/c8/Logit.svg" /></p>
<p><a href="https://en.wikipedia.org/wiki/File:Logit.svg" target="_blank">źródło</a></p>
<p>Zakładamy, że zmienna celu <span class="math notranslate nohighlight">\(y_i\)</span> przyjmuje wartości ze zbioru <span class="math notranslate nohighlight">\(\{0,1\}\)</span>, dla punktu <span class="math notranslate nohighlight">\(i\)</span>. Po dopasowaniu modelu, przewidujemy prawdopodobieństwa pozytywnych klas <span class="math notranslate nohighlight">\(P(y_i=1|X_i)\)</span> jako $<span class="math notranslate nohighlight">\(\hat{p}(X_i)=\frac{1}{1+\textrm{exp}(-X_iw-w_0)}.\)</span>$</p>
<p>Regresja logistyczna z regularyzacją <span class="math notranslate nohighlight">\(r(w)\)</span> minimalizuje poniższą funkcję:
$<span class="math notranslate nohighlight">\(\underset{w}{\textrm{min}}C\sum\limits^{n}_{i=1}(-y_i\textrm{log}(\hat{p}(X_i))-(1-y_i)\textrm{log}(1-\hat{p}(X_i)))+r(w)\)</span>$</p>
<p>Przykłady funkcji kary <span class="math notranslate nohighlight">\(r(w)\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(l_1\)</span>) <span class="math notranslate nohighlight">\(r(w)=||w||_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(l_2\)</span>) <span class="math notranslate nohighlight">\(r(w)=\frac{1}{2}||w||^2_2=\frac{1}{2}w^Tw\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definiujemy model klasyfikujący, bazujący na regresji logistycznej.  </span>
<span class="n">LR_clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span> <span class="o">=</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span>                  <span class="c1"># funkcja kary</span>
                            <span class="n">C</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>                         <span class="c1"># odwrotność siły regularyzacji, mniejsza wartość oznacza silniejszą regularyzacje</span>
                            <span class="n">fit_intercept</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>            <span class="c1"># oznacza, czy ma być dodany bias do funkcji decyzyjnej</span>
                            <span class="n">class_weight</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mf">0.7</span><span class="p">},</span>   <span class="c1"># wagi powiązane z klasami, parametr przekazywany w formie słownika</span>
                            <span class="n">solver</span> <span class="o">=</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">,</span>            <span class="c1"># algorytm używany w problemie optymalizacji, </span>
                                                             <span class="c1"># dla małych zbiorów dobry wyborem jest &#39;liblinear&#39;, natomiast &#39;sag&#39; i &#39;saga&#39; są szybsze dla większych zbiorów</span>
                                                             <span class="c1"># nie każdy wybór jest kompatybilny z funkcją kary</span>
                            <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>                  <span class="c1"># maksymalna liczba iteracji</span>
                            <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>

<span class="c1"># Uczymy powyższy model na naszych danych</span>
<span class="n">LR_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Stosujemy powyższy model na danych testowych</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">LR_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Wizualizacja działania algorytmu</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">]</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred</span>
<span class="n">sns</span><span class="o">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y_pred&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Wykres punktowy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/117d0d34d89cbdd2fd9879c55cefeffe1f39e47ab87ca40eae066392ba940a79.png" src="../_images/117d0d34d89cbdd2fd9879c55cefeffe1f39e47ab87ca40eae066392ba940a79.png" />
</div>
</div>
<p>W celu sprawdzenia działania powyższego algorytmu ponownie możemy posłużyć się metrykami opisanymi w osobnym rozdziale.</p>
</section>
<section id="bibliografia-logistic-regression">
<h2>Bibliografia Logistic Regression<a class="headerlink" href="#bibliografia-logistic-regression" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression</a></p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a></p>
</section>
<hr class="docutils" />
<section id="decision-trees">
<h2>Decision Trees<a class="headerlink" href="#decision-trees" title="Permalink to this heading">#</a></h2>
<p>Drzewa decyzyjne nadają się zarówno do klasyfikacji, jak i regresji. Przyjmują one dowolne typy danych, numeryczne i kategorialne, bez założeń dotyczących rozkładu i bez potrzeby ich wstępnego przetwarzania. Algorytm ten jest względnie łatwy w użyciu, a jego wyniki są w miare prostę w interpretacji. Po dopasowaniu modelu, przewidywanie wyników jest szybkim procesem. Jednak drzewa decyzyjne mają też swoje wady, mają one tendencję do przeuczania (zwłaszcza, gdy nie są przycinane).</p>
<p><strong>Budowa drzewa decyzyjnego</strong></p>
<p>Przykład drzewa decyzyjnego. Drzewo decyzyjne składa się z węzłów i gałęzi. Konstrukcję drzewa zaczynamy od korzenia, czyli pierwszego węzła (w przykładzie poniżej jest to “Warunek 1”). Następnie tworzymy gałęzie odpowiadające różnym możliwością spełnienia pierwszego warunku. W ten sposób powstają 3 kolejne węzły (“Tak”, “Warunek 2”, “Nie”). Węzły, od których nie rozchodzą się już kolejne gałęzie nazywamy liścmi, w zaprezentowanym przykładzie, są to wszystkie węzły z wartościami “Tak” i “Nie”.</p>
<!-- <img src="DT.png" alt="DT" style="width: 400px;">
 -->
<p><img alt="DT" src="../_images/DT.png" /></p>
<p><strong>Działanie algorytmu budującego drzewo decyzyjne</strong></p>
<p>Istnieją różne algorytmy budujące drzewa decyzyjne, są to np.</p>
<ul class="simple">
<li><p>ID3</p></li>
<li><p>C4.5</p></li>
<li><p>CART</p></li>
<li><p>CHAID</p></li>
<li><p>MARS</p></li>
</ul>
<p><em>Etapy działania algorytmu C4.5</em></p>
<ol class="arabic simple">
<li><p>Wybór zbioru danych z podziałem na zmienne objaśniające i zmienną objaśnianą,</p></li>
<li><p>policzenie metryki <em>Information Gain</em>, która pomaga stwierdzić, które zmienne w zbiorze treningowym są najużyteczniejsze w rozdzielaniu klas zmiennej celu oraz <em>entropii</em>,</p></li>
<li><p>Wybranie zmiennej z najwyższym Information Gain i uznanie jej za węzeł decyzyjny w drzewie,</p></li>
<li><p>policzenie Information Gain dla pozostałych zmiennych,</p></li>
<li><p>stworzenie węzłow wychodzących od węzła decyzyjnego,</p></li>
<li><p>powtarzanie powyższych kroków, dopóki wszystkie atrybuty nie zostaną użyte,</p></li>
<li><p>przycięcie drzewa w celu zapobiegnięcia przeuczeniu.</p></li>
</ol>
<p><strong>Przycinanie liści</strong></p>
<p>Żeby zapobiec zbyt dużemu rozrostowi drzewa decyzyjnego, który może doprowadzić do małego poziomu generalizacji oraz spowolnienia działania algorytmu, stosuje się tak zwane przycianie drzewa (ang <em>pruning</em>). Polega ono na usuwaniu zbędnych elementów z drzewa po jego utworzeniu.
Wyróżnia się dwa podstawowe rodzaje przycinania:</p>
<ol class="arabic simple">
<li><p>przycinanie wsteczne, polegające na wygenerowaniu drzewa, które jest bardzo dobrze dopasowane do zbioru treningowego, a następnie usuwanie od dołu najmniej efektywnych węzłów,</p></li>
<li><p>przycinanie w przód, polegające na wstrzymaniu dalszej rozbudowy danej gałęzi jeśli na węźle znajduje się ilość próbek zaklasyfikowanych do danej klasy, przekracza wyznaczony próg.</p></li>
</ol>
<p><strong>Miary podziału drzewa</strong></p>
<ul class="simple">
<li><p>Entropia - miara ilości informacji.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ E = -\sum\frac{|C_i|}{|X|}log\frac{|C_i|}{|X|}\]</div>
<p><span class="math notranslate nohighlight">\(C_i\)</span> - przykłady danej klasy</p>
<p>X - wszystkie przykłady</p>
<ul class="simple">
<li><p>Information Gain - ilość pozyskanej informacji w węzłach przed ich rozdzieleniem, miara ta mówi nam jak istotna jest zmienna. Jest to oczekiwana redukcja entropii zmiennej <span class="math notranslate nohighlight">\(X\)</span> osiągana za pomocą uczenia się stanu zmiennej losowej <span class="math notranslate nohighlight">\(Y\)</span>. Im większe Information Gain, tym tracimy na entropii.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(X,Y)=E(X)-E(X|Y)=E(Y)-H(Y|X)\]</div>
<div class="math notranslate nohighlight">
\[I=1-E\]</div>
<p><span class="math notranslate nohighlight">\(E(X|Y)\)</span> - entropia warunkowa</p>
<ul class="simple">
<li><p>Indeks Giniego, miara koncentracji (nierównomierności) rozkładu zmiennej losowej.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[gini(X)=1-\sum(\frac{|C_i|}{|X|})^2\]</div>
<p>Przyjmuje wartośći ze zbioru <span class="math notranslate nohighlight">\([0,1]\)</span>. Przy czym gini = 0 oznacza, że wszystkie obiekty należą do danej klasy, a wzrost wartości współczynnika oznacza wzrost nierówności rozkładu.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definiujemy model klasyfikujący, bazujący na drzewach decyzyjnych.  </span>
<span class="n">DT_clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>              <span class="c1"># maksymalna głebokość drzewa</span>
                                <span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;gini&#39;</span><span class="p">,</span>         <span class="c1"># funkcja mierząca jakość rozdzielenia węzła</span>
                                <span class="n">splitter</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span>          <span class="c1"># strategia wyboru podziału w każdym węźle, &quot;best&quot; dla najlepszego podziałi i &quot;random&quot; dla losowego</span>
                                <span class="n">min_samples_split</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>      <span class="c1"># minimalna ilość próbek potrzebna do dokonania podziałi węzła</span>
                                <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>       <span class="c1"># minimalna liczba próbek wymagana w liściu</span>
                                <span class="n">max_features</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>        <span class="c1"># maksymalna liczba rozważanych zmiennych podczas szukania najlepszego podziału węzła</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Uczymy powyższy model na naszych danych</span>
<span class="n">DT_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Stosujemy powyższy model na danych testowych</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">DT_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Wizualizacja działania algorytmu</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">]</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred</span>
<span class="n">sns</span><span class="o">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y_pred&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Wykres punktowy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/aa817a005f960d1ab5f975a59865f8255d1a01a878159fdd08e64780d431f42f.png" src="../_images/aa817a005f960d1ab5f975a59865f8255d1a01a878159fdd08e64780d431f42f.png" />
</div>
</div>
<p>Działanie algorytmu możemy zwizualizować w formie tekstowej, jak i graficznie.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">export_text</span><span class="p">(</span><span class="n">DT_clf</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">DT_clf</span><span class="p">,</span>         <span class="c1"># dodanie fragmentu _= sprawia, że nie wyświetlany jest tekst nad rysunkiem drzewa decyzyjnego, który odpowiada wartościom w liściach i węzłach drzew</span>
                   <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>|--- feature_1 &lt;= 0.23
|   |--- feature_1 &lt;= -0.23
|   |   |--- feature_1 &lt;= -0.56
|   |   |   |--- class: 0
|   |   |--- feature_1 &gt;  -0.56
|   |   |   |--- class: 0
|   |--- feature_1 &gt;  -0.23
|   |   |--- feature_1 &lt;= 0.10
|   |   |   |--- class: 0
|   |   |--- feature_1 &gt;  0.10
|   |   |   |--- class: 0
|--- feature_1 &gt;  0.23
|   |--- feature_1 &lt;= 0.35
|   |   |--- feature_0 &lt;= -1.67
|   |   |   |--- class: 1
|   |   |--- feature_0 &gt;  -1.67
|   |   |   |--- class: 0
|   |--- feature_1 &gt;  0.35
|   |   |--- feature_1 &lt;= 0.68
|   |   |   |--- class: 1
|   |   |--- feature_1 &gt;  0.68
|   |   |   |--- class: 1
</pre></div>
</div>
<img alt="../_images/c1f2e239266e769aa00711ecf26362ca76afe6094ea93e6e609635d3f4455ec4.png" src="../_images/c1f2e239266e769aa00711ecf26362ca76afe6094ea93e6e609635d3f4455ec4.png" />
</div>
</div>
</section>
<section id="bibliografia-decision-trees">
<h2>Bibliografia Decision Trees<a class="headerlink" href="#bibliografia-decision-trees" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier</a></p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/tree.html#tree">https://scikit-learn.org/stable/modules/tree.html#tree</a></p>
<p><a class="reference external" href="https://stackabuse.com/decision-trees-in-python-with-scikit-learn/">https://stackabuse.com/decision-trees-in-python-with-scikit-learn/</a></p>
<p><a class="reference external" href="https://pl.wikipedia.org/wiki/Drzewo_decyzyjne">https://pl.wikipedia.org/wiki/Drzewo_decyzyjne</a></p>
<p><a class="reference external" href="https://medium.com/analytics-vidhya/decision-trees-explained-in-simple-steps-39ee1a6b00a2">https://medium.com/analytics-vidhya/decision-trees-explained-in-simple-steps-39ee1a6b00a2</a></p>
</section>
<hr class="docutils" />
<section id="random-forests">
<h2>Random Forests<a class="headerlink" href="#random-forests" title="Permalink to this heading">#</a></h2>
<p>Lasy losowe przydają sie tam gdzie mamy do czynienia z dużą liczbą cech, można je określić jako uogólnienie drzew decyzyjnych. Ich działanie polega na klasyfikacji przy pomocy grupy drzew decyzyjnych (stąd nazwa las), a ostateczny wynik jest podejmowany za pomocą głosowania na tę cechę, która częściej pojawiała się dla drzew decyzyjnych.</p>
<p>Metodę działania lasów losowych można przedstawić w paru krokach:</p>
<ul class="simple">
<li><p>losowanie ze zwracaniem podzbioru danych z dostępnego zbioru treningowego</p></li>
<li><p>stworzenie drzewa decyzyjnego dla każdego podzbioru</p></li>
<li><p>predykcja następuje poprzez wybranie cechy, którą częściej wskazywały wszystkie drzewa decyzyjne</p></li>
</ul>
<p>Innymi słowy, gdy dla zmiennej testowej “T”, kiedy las składa się z 11 drzew i 3 drzewa uznają, że zmienna ta powinna zostać oznaczona jako 0, a 8 drzew przypisze jej klasę 1, to ostatecznie zostanie ona zakwalifikowana jako 1.
Zaletą lasów losowych jest to, że są one odporne na braki danych, różne typy zmiennych czy istnienie wartości odstających. Są one odporne na przeuczenie oraz zachowują stabilność.</p>
<p>Ponieważ lasy losowe korzystają z drzew decyzyjnych, które powstają na podstawie różnych podzbiorów zmiennych objaśniających, w pierwszym kroku wygenerujemy nowy zbiór danych zawierający większą liczbę zmiennych. Mając teraz 6 zmiennych objaśniających, nie jesteśmy w stanie przedstawić ich na pojedynczym wykresie. Dlatego w celu zobrazowania jak wygląda nasz zbiór posłuzymy się funkcją pairplot, przedstawi nam ona zależności pomiędzy każdymi dwiema zmiennymi.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stworzenie losowego obiektu np.array</span>
<span class="n">X_RF</span><span class="p">,</span> <span class="n">y_RF</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>  <span class="c1"># ilość danych</span>
                           <span class="n">n_features</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>    <span class="c1"># liczba zmiennych objaśniających</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="c1"># liczba &#39;użytecznych&#39; zmiennych</span>
                           <span class="n">n_redundant</span> <span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># liczba zmiennych &#39;nieużytecznych&#39;</span>
                           <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>     <span class="c1"># liczba klas</span>
                           <span class="n">class_sep</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>     <span class="c1"># liczba określająca jak bardzo klasy powinny być od siebie odseparowane</span>
                           <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>   <span class="c1"># zbalansowanie danych</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_RF</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">,</span><span class="s1">&#39;X3&#39;</span><span class="p">,</span> <span class="s1">&#39;X4&#39;</span><span class="p">,</span> <span class="s1">&#39;X5&#39;</span><span class="p">,</span> <span class="s1">&#39;X6&#39;</span><span class="p">]</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_RF</span>

<span class="c1"># Sprawdzenie czy dane na pewno są zbalansowane</span>
<span class="n">display</span><span class="p">(</span><span class="s2">&quot;Ilość poszczególnych wartości zmiennej y&quot;</span><span class="p">,(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">30</span><span class="p">)</span>

<span class="c1"># Wizualizacja powyższego zbioru danych z wyróżnieniem zmiennej y</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Ilość poszczególnych wartości zmiennej y&#39;
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y
0    1395
1     605
Name: count, dtype: int64
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>------------------------------
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 1500x1500 with 0 Axes&gt;
</pre></div>
</div>
<img alt="../_images/08018706b4ffe9b6cde2a50c22839a73fff87250c043160a820fcb696e3fb724.png" src="../_images/08018706b4ffe9b6cde2a50c22839a73fff87250c043160a820fcb696e3fb724.png" />
</div>
</div>
<p>Jak widać na powyższym wykresie, otrzymaliśmy niezbalansowany zbiór danych względnie podzielony na 2 klasy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dzielimy nasze dane na zbiory uczący i testowy</span>
<span class="n">X_RF_train</span><span class="p">,</span> <span class="n">X_RF_test</span><span class="p">,</span> <span class="n">y_RF_train</span><span class="p">,</span> <span class="n">y_RF_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_RF</span><span class="p">,</span> <span class="n">y_RF</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>W modelu lasów losowych możemy zastosować metodę <em>bootstrapu</em>, która polega na tym, że zamiast trenować model na całym zbiorze danych, to każde drzewo w lesie losowym jest tworzone na podstawie podzbioru zbioru obserwacji. Następnie rezultaty są agregowane. Bootstrap stosuje się by zapewnić różnorodność w lasach losowych, pomaga to w zapobieganiu przeuczania się modelu oraz redukuje wariancję w predykcjach, jednak dostajemy pewien bias w każdym drzewie na skutek użycia mniejszej ilości danych do ich stworzenia.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definiujemy model klasyfikujący, bazujący na lasach losowych.</span>
<span class="n">RF_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>          <span class="c1"># liczba drzew w lesie</span>
                                <span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;gini&#39;</span><span class="p">,</span>         <span class="c1"># funkcja mierząca jakość rozdzielenia węzła</span>
                                <span class="n">min_samples_split</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>      <span class="c1"># minimalna ilość próbek potrzebna do dokonania podziałi węzła</span>
                                <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>       <span class="c1"># minimalna liczba próbek wymagana w liściu</span>
                                <span class="n">max_features</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>        <span class="c1"># maksymalna liczba rozważanych zmiennych podczas szukania najlepszego podziału węzła</span>
                                <span class="n">bootstrap</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>           <span class="c1"># określa czy próbki bootstrapowe są stosowane, gdy ustawione na False, to cały zbiór jest</span>
                                                            <span class="c1"># uwzględniany podczas tworzenia każdego drzewa</span>
                                <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>                <span class="c1"># maksymalna głębokość pojedynczego drzewa</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Uczymy powyższy model na naszych danych</span>
<span class="n">RF_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_RF_train</span><span class="p">,</span> <span class="n">y_RF_train</span><span class="p">)</span>

<span class="c1"># Stosujemy powyższy model na danych testowych</span>
<span class="n">y_RF_pred</span> <span class="o">=</span> <span class="n">RF_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_RF_test</span><span class="p">)</span>

<span class="c1"># Wizualizacja działania algorytmu</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_RF_test</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">,</span><span class="s1">&#39;X3&#39;</span><span class="p">,</span> <span class="s1">&#39;X4&#39;</span><span class="p">,</span> <span class="s1">&#39;X5&#39;</span><span class="p">,</span> <span class="s1">&#39;X6&#39;</span><span class="p">]</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_RF_pred</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y_pred&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 1500x1500 with 0 Axes&gt;
</pre></div>
</div>
<img alt="../_images/f0420fc2946cdbe8ea4f08ed6f6a8f2bdc88ed5e7ad4171bd43cbae8df07fd44.png" src="../_images/f0420fc2946cdbe8ea4f08ed6f6a8f2bdc88ed5e7ad4171bd43cbae8df07fd44.png" />
</div>
</div>
<p>Po zastosowaniu algorytmu lasów losowych, widzimy jak zmiennę w miarę poprawnie zostały oznaczone jako dana klasa. Dodatkowo możemy też sprawdzić jak wygląda każde drzewo w lesie, poniżej zaprezentowano pierwsze z 20 drzew.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Wykres pierwszego z 20 drzew w lesie losowym.</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">RF_clf</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                   <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d7bd1386df1b19aa1dfa692033d9e9f1dc6116cbb1ec2026c7ac5b7b14158c57.png" src="../_images/d7bd1386df1b19aa1dfa692033d9e9f1dc6116cbb1ec2026c7ac5b7b14158c57.png" />
</div>
</div>
</section>
<section id="bibliografia-random-forest">
<h2>Bibliografia Random Forest<a class="headerlink" href="#bibliografia-random-forest" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/ensemble.html#forest">https://scikit-learn.org/stable/modules/ensemble.html#forest</a></p>
<p><a class="reference external" href="https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/">https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/</a></p>
</section>
<hr class="docutils" />
<section id="support-vector-machines">
<h2>Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this heading">#</a></h2>
<p>Pewne problemy klasyfikacji są nierozdzielne, tzn. przykłady z klasy 1 znajdują się w obszarze otoczonym przez przykłady z klasy 0, co uniemożliwia rozdzielenie obydwu klas za pomocą prostej granicy. Z tego powodu metody liniowe nie są w stanie całkowicie rozdzielić dwóch klas.</p>
<p>Maszyny wektorów nośnych radzą sobie z tym problemem, jednak bez wyboru odpowiedniego jądra mogą one nie dawać najlepszych rezultatów.</p>
<p>Maszyna wektorów nośnych opiera się na stworzeniu lini pomiędzy różnymi skupiskami danych, by następnie pogrupować je w klasy. Punkty po jednej stronie linii będą należeć do jednej klasy, a punkty po drugiej do innej klasy. Algorytm próbuje zmaksymalizować odległość pomiędzy linią, którą wyznacza, a punktami na jej zewnętrzu. Następnie wartości testowe klasyfikowane są przy pomocy tych lini.</p>
<p>Zaletami SVM jest ich skuteczność w przestrzeniach wielowymiarowych, wciąż sprawdzają się tam gdzie wymiar przestrzeni jest większy niż liczba próbek, nie zużywają dużo pamięci oraz można dopasować samodzielnie zdefiniowane jądro do algorytmu. Efekty zastosowania różnego rodzaju jądra na tym samym zbiorze danych przedstawiono na poniższym rysunku.</p>
<!-- <img src="kernels.png" alt="DT" style="width: 800px;">
 -->
<p><img alt="media" src="../_images/kernels.png" /></p>
<p><a href="https://scikit-learn.org/stable/modules/svm.html#svm-classification" target="_blank">źródło</a></p>
<p><strong>Polynomial Kernel Function</strong></p>
<p>Jądra wielomianowe są uogólnioną reprezentacją jąder o stopniu większym niż 1. Są przydatne w przetwarzaniu obrazów.</p>
<p>Istnieją dwa typy jąder wielomianowych:</p>
<ol class="arabic simple">
<li><p>homogeniczne jądra wielomianowe</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ K(x_i,x_j)=(x_i\cdot x_j),\]</div>
<p>gdzie <span class="math notranslate nohighlight">\(\cdot\)</span> oznacza iloczyn skalarny obu wektorów, a <span class="math notranslate nohighlight">\(d\)</span> jest stopniem wielomianu.</p>
<ol class="arabic simple" start="2">
<li><p>niehomogeniczne jądra wielomianowe</p></li>
</ol>
<div class="math notranslate nohighlight">
\[K(x_i,x_j)=(x_i\cdot x_j+c)^d,\]</div>
<p>gdzie <span class="math notranslate nohighlight">\(c\)</span> jest jakąś stałą.</p>
<p><strong>Gaussian Radial Basis Function (RBF) Kernel</strong></p>
<p>Jądra radialnego używa się, gdy nie mamy dużej wiedzy na temat danych. Wyraża się je za pomocą wzoru</p>
<div class="math notranslate nohighlight">
\[K(x_i,x_j)=\textrm{exp}(-\frac{||x_i-x_j||}{2 \sigma ^2})^2,\]</div>
<p>gdzie <span class="math notranslate nohighlight">\(\sigma\)</span> oznacza wariancje, a <span class="math notranslate nohighlight">\(||x_i-x_j||\)</span> określa odległość euklidesową pomiędzy dwoma punktami.</p>
<p><strong>Linear Kernel Function</strong></p>
<p>Jest to jądro jednowymiarowe o najprostszej formie. Określa je poniższy wzór</p>
<div class="math notranslate nohighlight">
\[K(x_i,x_j)=x_i\cdot x_j+c\]</div>
<p>W celu zaprezentowania metody działania algorytmu SVM, stworzymy zbiór, który będzię posiadał punkty odpowiadające jednej klasie, zagnieżdżone w zbiorze z drugiej klasy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="n">X_svm</span><span class="p">,</span> <span class="n">y_svm</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>  <span class="c1"># ilość danych</span>
                          <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>    <span class="c1"># poziom szumu</span>

<span class="c1"># Wizualizacja powyższego zbioru danych z wyróżnieniem zmiennej y</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_svm</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">]</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_svm</span>
<span class="n">sns</span><span class="o">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Wykres punktowy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7d491cd4bb57a5df87e50fd22bdf5053c6e5a7e229bf96e279cf00c24eb0ad72.png" src="../_images/7d491cd4bb57a5df87e50fd22bdf5053c6e5a7e229bf96e279cf00c24eb0ad72.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dzielimy nasze dane na zbiory uczący i testowy</span>
<span class="n">X_svm_train</span><span class="p">,</span> <span class="n">X_svm_test</span><span class="p">,</span> <span class="n">y_svm_train</span><span class="p">,</span> <span class="n">y_svm_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_svm</span><span class="p">,</span> <span class="n">y_svm</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="przyklad-z-jadrem-liniowym">
<h3>Przykład z jądrem liniowym.<a class="headerlink" href="#przyklad-z-jadrem-liniowym" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definiujemy model klasyfikujący, bazujący na algorytmie support vector machines.</span>
<span class="n">SVM_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>               <span class="c1"># parametr określający złożoność modelu</span>
              <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span>     <span class="c1"># typ jądra</span>
              <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>  

<span class="c1"># Uczymy powyższy model na naszych danych</span>
<span class="n">SVM_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_svm_train</span><span class="p">,</span> <span class="n">y_svm_train</span><span class="p">)</span>

<span class="c1"># Stosujemy powyższy model na danych testowych</span>
<span class="n">y_svm_pred</span> <span class="o">=</span> <span class="n">SVM_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_svm_test</span><span class="p">)</span>

<span class="c1"># Wizualizacja działania algorytmu</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_svm_test</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">]</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_svm_pred</span>
<span class="n">sns</span><span class="o">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y_pred&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Wykres punktowy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/120b467fe7af8fb25ad4ed0bb59e9e7584e32e2014da356a2588ebbf734bb5ba.png" src="../_images/120b467fe7af8fb25ad4ed0bb59e9e7584e32e2014da356a2588ebbf734bb5ba.png" />
</div>
</div>
</section>
<section id="przyklad-z-jadrem-radialnym">
<h3>Przykład z jądrem radialnym.<a class="headerlink" href="#przyklad-z-jadrem-radialnym" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definiujemy model klasyfikujący, bazujący na algorytmie support vector machines.</span>
<span class="n">SVM_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>               <span class="c1"># parametr określający złożoność modelu</span>
              <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span>        <span class="c1"># typ jądra</span>
              <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>  

<span class="c1"># Uczymy powyższy model na naszych danych</span>
<span class="n">SVM_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_svm_train</span><span class="p">,</span> <span class="n">y_svm_train</span><span class="p">)</span>

<span class="c1"># Stosujemy powyższy model na danych testowych</span>
<span class="n">y_svm_pred</span> <span class="o">=</span> <span class="n">SVM_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_svm_test</span><span class="p">)</span>

<span class="c1"># Wizualizacja działania algorytmu</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_svm_test</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">]</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_svm_pred</span>
<span class="n">sns</span><span class="o">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>  <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y_pred&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Wykres punktowy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b50a5d2096ee96de46c96702a4f6cba4f883be1db0d56b56554b0d972f6c6a56.png" src="../_images/b50a5d2096ee96de46c96702a4f6cba4f883be1db0d56b56554b0d972f6c6a56.png" />
</div>
</div>
<p>Na pierwszym przykładzie widać, że jądro liniowe nie do końca się sprawdza. Jednak po zastosowaniu jądra radialnego, rezultaty są o wiele lepsze. Maszyny wektorów nośnych z jądrem radialnym są bardzo dobrymi klasyfikatorami rozpoznającymi najbliższych sąsiadów.</p>
</section>
</section>
<section id="bibliografia-support-vector-machines">
<h2>Bibliografia Support Vector Machines<a class="headerlink" href="#bibliografia-support-vector-machines" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC</a></p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/svm.html#svm-classification">https://scikit-learn.org/stable/modules/svm.html#svm-classification</a></p>
<p><a class="reference external" href="https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/">https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/</a></p>
</section>
<hr class="docutils" />
<section id="dodatkowe-informacje">
<h2>Dodatkowe informacje<a class="headerlink" href="#dodatkowe-informacje" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py">https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py</a></p></li>
<li><p><a class="reference external" href="https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/">https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/</a></p></li>
<li><p><a class="reference external" href="https://vitalflux.com/classification-problems-real-world-examples/">https://vitalflux.com/classification-problems-real-world-examples/</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./12. Klasyfikacja"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../7.%20Klasteryzacja/7_Klasteryzacja.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Klasteryzacja w Python</p>
      </div>
    </a>
    <a class="right-next"
       href="../11.%20Regresja/Regresja_lin.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Liniowe modele regresji</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wstep">Wstęp</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#macierz-pomylek">Macierz pomyłek</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#biblioteki">Biblioteki</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#przygotowanie-danych">Przygotowanie danych</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#losowe-dane">Losowe dane</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia">Bibliografia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modele-klasyfikacyjne">Modele klasyfikacyjne</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors">K-Nearest Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#przyklad">Przykład</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-k-nearest-neighbors">Bibliografia K-Nearest Neighbors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-logistic-regression">Bibliografia Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-trees">Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-decision-trees">Bibliografia Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forests">Random Forests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-random-forest">Bibliografia Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machines">Support Vector Machines</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#przyklad-z-jadrem-liniowym">Przykład z jądrem liniowym.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#przyklad-z-jadrem-radialnym">Przykład z jądrem radialnym.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bibliografia-support-vector-machines">Bibliografia Support Vector Machines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dodatkowe-informacje">Dodatkowe informacje</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By codersi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>